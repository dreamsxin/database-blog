## PostgreSQL 开启“审计日志、时间记录”带来的性能影响有多少？     
                                                             
### 作者                                                             
digoal                                                             
                                                             
### 日期                                                             
2018-06-16                                                           
                                                             
### 标签                                                             
PostgreSQL , 审计日志 , duration , 性能影响 , syslogger , BUFFER , log_statement , log_duration , track_io_timing , osq_lock           
                                                             
----                                                             
                                                             
## 背景       
开启审计日志，或者开启数据库的SQL耗时记录，会给数据库带来多大的性能开销？    
    
1、审计日志通过log_statement设置，可以设置为all, ddl, mod, none，分别表示审计所有SQL，DDL SQL，DML SQL，不审计。    
    
开启审计时，在执行SQL的时候（提交SQL请求时），数据库日志中打印类似这样的信息：    
    
```    
2018-06-16 14:48:23.760 CST,"postgres","postgres",19448,"[local]",5b24b270.4bf8,8,"idle",2018-06-16 14:47:12 CST,3/1484181,0,LOG,00000,"statement: select count(*) from pg_class;",,,,,,,,"exec_simple_query, postgres.c:940","psql"    
```    
    
2、SQL耗时记录，通过设置log_duration参数来指定，设置为ON 或OFF。    
    
如果打开时间记录，那么在SQL执行结束时，打印SQL的耗时。（不包括将结果传输到客户端的时间）    
    
```    
2018-06-16 14:48:23.760 CST,"postgres","postgres",19448,"[local]",5b24b270.4bf8,9,"SELECT",2018-06-16 14:47:12 CST,3/0,0,LOG,00000,"duration: 0.609 ms",,,,,,,,"exec_simple_query, postgres.c:1170","psql"    
```    
    
## 开启这两个开关，有多大的性能开销？    
    
1、开启审计，实际上性能开销较小，因为日志是异步管道输出，而且目前的BUFFER已经调得比较大，性能损耗真的非常小。    
    
src/backend/postmaster/syslogger.c    
    
```    
/*    
 * Buffers for saving partial messages from different backends.    
 *    
 * Keep NBUFFER_LISTS lists of these, with the entry for a given source pid    
 * being in the list numbered (pid % NBUFFER_LISTS), so as to cut down on    
 * the number of entries we have to examine for any one incoming message.    
 * There must never be more than one entry for the same source pid.    
 *    
 * An inactive buffer is not removed from its list, just held for re-use.    
 * An inactive buffer has pid == 0 and undefined contents of data.    
 */    
typedef struct    
{    
        int32           pid;                    /* PID of source process */    
        StringInfoData data;            /* accumulated data, as a StringInfo */    
} save_buffer;    
    
#define NBUFFER_LISTS 256    
static List *buffer_lists[NBUFFER_LISTS];    
```    
    
2、开启时间统计，这个开销主要取决于读取时钟的额外开销。  
    
通过pg_test_timing可以测试获取系统时间带来的额外开销。    
    
数据库中如果要统计IO TIMING(track_io_timing = on)，以及auto_explain中的log timing，开销也与之类似。    
    
```    
pg_test_timing -d 10    
Testing timing overhead for 10 seconds.    
Per loop time including overhead: 42.38 ns    
Histogram of timing durations:    
  < us   % of total      count    
     1     95.76853  225960491    
     2      4.22695    9973261    
     4      0.00430      10145    
     8      0.00005        109    
    16      0.00016        370    
    32      0.00001         32    
    64      0.00000          3    
```    
    
## 实测   
测试环境为ECS主机，测出了一个虚拟机的性能缺陷。  
  
1、创建测试数据     
    
```    
pgbench -i -s 300    
```    
    
2、测试小事务    
    
```    
pgbench -M prepared -v -r -P 1 -c 56 -j 56 -T 120 -S    
```    
    
3、关闭log_statement与log_duration    
    
```    
transaction type: <builtin: select only>    
scaling factor: 300    
query mode: prepared    
number of clients: 56    
number of threads: 56    
duration: 120 s    
number of transactions actually processed: 25058528    
latency average = 0.268 ms    
latency stddev = 0.217 ms    
tps = 208818.253316 (including connections establishing)    
tps = 208834.342644 (excluding connections establishing)    
script statistics:    
 - statement latencies in milliseconds:    
         0.025  \set aid random(1, 100000 * :scale)    
         0.252  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;    
```    
    
4、开启log_statement=all    
    
```    
transaction type: <builtin: select only>    
scaling factor: 300    
query mode: prepared    
number of clients: 56    
number of threads: 56    
duration: 120 s    
number of transactions actually processed: 23088368    
latency average = 0.291 ms    
latency stddev = 1.445 ms    
tps = 192401.524926 (including connections establishing)    
tps = 192413.921654 (excluding connections establishing)    
script statistics:    
 - statement latencies in milliseconds:    
         0.018  \set aid random(1, 100000 * :scale)    
         0.282  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;    
```    
    
5、关闭log_statement，开启log_duration=on    
    
```    
transaction type: <builtin: select only>    
scaling factor: 300    
query mode: prepared    
number of clients: 56    
number of threads: 56    
duration: 120 s    
number of transactions actually processed: 12842222    
latency average = 0.523 ms    
latency stddev = 1.516 ms    
tps = 107017.184761 (including connections establishing)    
tps = 107025.759343 (excluding connections establishing)    
script statistics:    
 - statement latencies in milliseconds:    
         0.019  \set aid random(1, 100000 * :scale)    
         0.520  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;    
```    
    
6、开启log_statement=all，同时开启log_duration=on    
    
```    
transaction type: <builtin: select only>    
scaling factor: 300    
query mode: prepared    
number of clients: 56    
number of threads: 56    
duration: 120 s    
number of transactions actually processed: 7484093    
latency average = 0.898 ms    
latency stddev = 1.386 ms    
tps = 62366.260357 (including connections establishing)    
tps = 62370.603530 (excluding connections establishing)    
script statistics:    
 - statement latencies in milliseconds:    
         0.016  \set aid random(1, 100000 * :scale)    
         0.897  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;    
```    
    
7、开启log_statement=all，同时开启log_duration=on，同时开启track_io_timing = on    
    
    
```    
transaction type: <builtin: select only>    
scaling factor: 300    
query mode: prepared    
number of clients: 56    
number of threads: 56    
duration: 120 s    
number of transactions actually processed: 7161693    
latency average = 0.938 ms    
latency stddev = 1.150 ms    
tps = 59679.955252 (including connections establishing)    
tps = 59684.540717 (excluding connections establishing)    
script statistics:    
 - statement latencies in milliseconds:    
         0.015  \set aid random(1, 100000 * :scale)    
         0.938  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;    
```    
  
## 诊断  
测试结果与实际的时钟开销不相符，因为时钟开销实际上只有几十纳秒，但是测试结果很显然不止这么多。  
  
profiling  
  
```  
Samples: 1M of event 'cpu-clock', Event count (approx.): 234709749074   
Overhead  Shared Object          Symbol                                 
  41.15%  [kernel]               [k] osq_lock                           
  19.00%  libc-2.17.so           [.] __mcount_internal                  
  10.37%  [kernel]               [k] _raw_spin_unlock_irqrestore  
```  
  
https://patchwork.kernel.org/patch/9200635/  
  
```  
An over-committed guest with more vCPUs than pCPUs has a heavy overload  
in osq_lock().  
  
This is because vCPU A hold the osq lock and yield out, vCPU B wait  
per_cpu node->locked to be set. IOW, vCPU B wait vCPU A to run and  
unlock the osq lock. Such spinning is meaningless.  
  
So lets use vcpu_is_preempted() to detect if we need stop the spinning  
  
test case:  
perf record -a perf bench sched messaging -g 400 -p && perf report  
  
before patch:  
18.09%  sched-messaging  [kernel.vmlinux]  [k] osq_lock  
12.28%  sched-messaging  [kernel.vmlinux]  [k] rwsem_spin_on_owner  
 5.27%  sched-messaging  [kernel.vmlinux]  [k] mutex_unlock  
 3.89%  sched-messaging  [kernel.vmlinux]  [k] wait_consider_task  
 3.64%  sched-messaging  [kernel.vmlinux]  [k] _raw_write_lock_irq  
 3.41%  sched-messaging  [kernel.vmlinux]  [k] mutex_spin_on_owner.is  
 2.49%  sched-messaging  [kernel.vmlinux]  [k] system_call  
  
after patch:  
20.68%  sched-messaging  [kernel.vmlinux]  [k] mutex_spin_on_owner  
 8.45%  sched-messaging  [kernel.vmlinux]  [k] mutex_unlock  
 4.12%  sched-messaging  [kernel.vmlinux]  [k] system_call  
 3.01%  sched-messaging  [kernel.vmlinux]  [k] system_call_common  
 2.83%  sched-messaging  [kernel.vmlinux]  [k] copypage_power7  
 2.64%  sched-messaging  [kernel.vmlinux]  [k] rwsem_spin_on_owner  
 2.00%  sched-messaging  [kernel.vmlinux]  [k] osq_lock  
  
Signed-off-by: Pan Xinhui <xinhui.pan@linux.vnet.ibm.com>  
---  
 kernel/locking/osq_lock.c | 16 +++++++++++++++-  
 1 file changed, 15 insertions(+), 1 deletion(-)  
Patch  
diff --git a/kernel/locking/osq_lock.c b/kernel/locking/osq_lock.c  
index 05a3785..9e86f0b 100644  
--- a/kernel/locking/osq_lock.c  
+++ b/kernel/locking/osq_lock.c  
@@ -21,6 +21,11 @@  static inline int encode_cpu(int cpu_nr)  
 	return cpu_nr + 1;  
 }  
   
+static inline int node_cpu(struct optimistic_spin_node *node)  
+{  
+	return node->cpu - 1;  
+}  
+  
 static inline struct optimistic_spin_node *decode_cpu(int encoded_cpu_val)  
 {  
 	int cpu_nr = encoded_cpu_val - 1;  
@@ -118,8 +123,17 @@  bool osq_lock(struct optimistic_spin_queue *lock)  
 	while (!READ_ONCE(node->locked)) {  
 		/*  
 		 * If we need to reschedule bail... so we can block.  
+		 * An over-committed guest with more vCPUs than pCPUs  
+		 * might fall in this loop and cause a huge overload.  
+		 * This is because vCPU A(prev) hold the osq lock and yield out  
+		 * vCPU B(node) wait ->locked to be set, IOW, it wait utill  
+		 * vCPU A run and unlock the osq lock. Such spin is meaningless  
+		 * use vcpu_is_preempted to detech such case. IF arch does not  
+		 * support vcpu preempted check, vcpu_is_preempted is a macro  
+		 * defined by false.  
 		 */  
-		if (need_resched())  
+		if (need_resched() ||  
+			vcpu_is_preempted(node_cpu(node->prev)))  
 			goto unqueue;  
   
 		cpu_relax_lowlatency();  
```  
  
把连接数降到物理核。  
  
3、关闭log_statement与log_duration    
  
  
```  
transaction type: <builtin: select only>  
scaling factor: 300  
query mode: prepared  
number of clients: 28  
number of threads: 28  
duration: 120 s  
number of transactions actually processed: 17770447  
latency average = 0.189 ms  
latency stddev = 0.077 ms  
tps = 148086.294302 (including connections establishing)  
tps = 148093.564454 (excluding connections establishing)  
script statistics:  
 - statement latencies in milliseconds:  
         0.013  \set aid random(1, 100000 * :scale)  
         0.176  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;  
```  
  
4、开启log_statement=all    
  
```  
transaction type: <builtin: select only>  
scaling factor: 300  
query mode: prepared  
number of clients: 28  
number of threads: 28  
duration: 120 s  
number of transactions actually processed: 17236212  
latency average = 0.195 ms  
latency stddev = 0.321 ms  
tps = 143634.319683 (including connections establishing)  
tps = 143642.034125 (excluding connections establishing)  
script statistics:  
 - statement latencies in milliseconds:  
         0.010  \set aid random(1, 100000 * :scale)  
         0.185  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;  
```  
  
5、关闭log_statement，开启log_duration=on    
  
```  
transaction type: <builtin: select only>  
scaling factor: 300  
query mode: prepared  
number of clients: 28  
number of threads: 28  
duration: 120 s  
number of transactions actually processed: 16774800  
latency average = 0.200 ms  
latency stddev = 0.977 ms  
tps = 139789.393297 (including connections establishing)  
tps = 139795.661393 (excluding connections establishing)  
script statistics:  
 - statement latencies in milliseconds:  
         0.008  \set aid random(1, 100000 * :scale)  
         0.193  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;  
```  
  
6、开启log_statement=all，同时开启log_duration=on    
  
```  
transaction type: <builtin: select only>  
scaling factor: 300  
query mode: prepared  
number of clients: 28  
number of threads: 28  
duration: 120 s  
number of transactions actually processed: 14696887  
latency average = 0.229 ms  
latency stddev = 0.277 ms  
tps = 122473.319859 (including connections establishing)  
tps = 122479.381454 (excluding connections establishing)  
script statistics:  
 - statement latencies in milliseconds:  
         0.007  \set aid random(1, 100000 * :scale)  
         0.224  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;  
```  
  
7、开启log_statement=all，同时开启log_duration=on，同时开启track_io_timing = on    
  
```  
transaction type: <builtin: select only>  
scaling factor: 300  
query mode: prepared  
number of clients: 28  
number of threads: 28  
duration: 120 s  
number of transactions actually processed: 14606445  
latency average = 0.230 ms  
latency stddev = 0.726 ms  
tps = 121719.147924 (including connections establishing)  
tps = 121725.278733 (excluding connections establishing)  
script statistics:  
 - statement latencies in milliseconds:  
         0.007  \set aid random(1, 100000 * :scale)  
         0.226  SELECT abalance FROM pgbench_accounts WHERE aid = :aid;  
```  
  
profiling  
  
```  
Samples: 672K of event 'cpu-clock', Event count (approx.): 146760374621   
Overhead  Shared Object        Symbol                                     
  29.73%  libc-2.17.so         [.] __mcount_internal                      
   6.37%  [kernel]             [k] _raw_spin_unlock_irqrestore            
   5.93%  libc-2.17.so         [.] _mcount                                
   4.18%  libc-2.17.so         [.] vfprintf                               
   2.96%  [kernel]             [k] osq_lock                               
   2.92%  [kernel]             [k] __do_softirq                           
   1.80%  [kernel]             [k] finish_task_switch                     
   1.34%  postgres             [.] hash_search_with_hash_value            
   1.20%  libc-2.17.so         [.] _IO_default_xsputn   
```  
    
## 小结    
开启审计，影响不大，因为审计日志是通过异步消息进行记录，并且有较大的缓冲区。    
    
开启时间统计，性能影响主要取决于时钟获取的额外开销。    
    
### 测试结果1:  (ECS vCPU over head的缺陷，导致性能下降验证)  
    
测试CASE | QPS    
---|---    
关闭审计，关闭时间统计 | 208834    
开启审计 | 192413    
开启SQL时间统计 | 107025    
同时开启审计，SQL时间统计 | 62370    
同时开启审计，SQL时间统计，IO时间统计 | 59684    
  
### 测试结果2:    
降低连接，避免触发ECS vCPU overhead的问题，实际上开启审计和时间统计，开销都比较小。  
    
测试CASE | QPS    
---|---    
关闭审计，关闭时间统计 | 148093    
开启审计 | 143642    
开启SQL时间统计 | 139795    
同时开启审计，SQL时间统计 | 122479    
同时开启审计，SQL时间统计，IO时间统计 | 121725    
    
    
## 参考    
https://www.postgresql.org/docs/11/static/pgtesttiming.html    
    
[《Linux 时钟精度 与 PostgreSQL auto_explain (explain timing 时钟开销估算)》](../201612/20161228_02.md)      
    
  
  
<a rel="nofollow" href="http://info.flagcounter.com/h9V1"  ><img src="http://s03.flagcounter.com/count/h9V1/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_12/viewers_0/labels_0/pageviews_0/flags_0/"  alt="Flag Counter"  border="0"  ></a>  
  
