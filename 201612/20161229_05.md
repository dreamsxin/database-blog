## 轻松打爆netfilter conntrack table的Greenplum SQL  
    
### 作者    
digoal    
    
### 日期    
2016-12-29    
    
### 标签    
PostgreSQL , Greenplum , udp , 重分布 , 内部链接     
    
----    
    
## 背景    
Greenplum 是一款MPP数据库，数据哈希打散存放在数据节点，当执行JOIN时，如果JOIN字段非分布键，那么数据会在数据节点之间重分布。重分布使用四阶段方法，参考如下：  
  
[《分布式DB(Greenplum)中数据倾斜的原因和解法 - 阿里云HybridDB for PostgreSQL最佳实践》](../201708/20170821_02.md)    
  
当数据重分布时，每个数据节点之间都会建立一对连接（默认为UDP连接），当数据节点很多时，这个内部连接可能特别多。  
  
![pic](20161229_05_pic_001.jpg)  
  
例如：  
  
1024个segment的Greenplum集群，当某个QUERY涉及数据重分布时，每个segment会产生1023*2=2046个UDP连接(包括连接其他节点，以及其他节点连接自己的)。  
  
假设一台物理机有32个segment，那么这个主机会产生2046*32=65472个连接。假设有100个并发请求都产生了数据重分布，那么这台主机会产生6547200个连接。  
  
如果LINUX主机开启了iptables防火墙，并且开启了会话跟踪，那么会话跟踪表会被轻松打爆。导致报错：  
  
```  
dmesg  
  
nf_conntrack: table full, dropping packet  
```  
  
从而影响性能。  
  
## 解决办法  
设置较大的nf_conntrack_max LINUX内核参数，可以缓解```nf_conntrack: table full, dropping packet```的问题。  
  
比较好的方法是：  
  
1、不启用nf_conntrack模块。  
  
2、使用raw表，关闭跟踪。  
  
具体的方法请参考末尾文章。  
  
3、第三种方法，在部署Greenplum时，每个主机不要部署太多的segment，从而降低单个主机的会话数。   
  
4、第四种方法，使用连接池，比如pgbouncer，减少连到GP的实际连接。   
  
[《Greenplum 连接池实践》](../201801/20180128_04.md)  
  
## 参考    
[《netfilter内核模块知识 - 解决nf_conntrack: table full, dropping packet》](../201612/20161229_04.md)    
    
[《[转载]解决 nf_conntrack: table full, dropping packet 的几种思路》](../201612/20161229_03.md)    
    
[《转载 - nf_conntrack: table full, dropping packet. 终结篇》](../201612/20161229_02.md)    
