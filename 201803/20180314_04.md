## PostgreSQL 流计算插件pipelinedb sharding 集群版原理介绍 - 一个全功能的分布式流计算引擎 
                                                                 
### 作者                                                                 
digoal                                                                 
                                                                 
### 日期                                                                 
2018-03-14                                                               
                                                                 
### 标签                                                                 
PostgreSQL , pipelinedb , 流计算 , sharding , 水平扩展   
                                                                 
----                                                                 
                                                                 
## 背景        
  
pipelinedb cluster定位是一个分布式流式计算引擎。  
  
## pipelinedb cluster架构  
  
![pic](20180314_04_pic_001.jpg)  
  
为了达到无master的架构，pipelinedb集群的所有节点间互相信任通信，实现CV中间聚合结果的数据自动分发，除STREAM，CV以外的其他普通数据DML DDL的全分发以及2PC提交。  
  
由于是无master架构，客户端连接任意pipelinedb shared节点都可以。  
  
CV以外的其他DML DDL操作，2PC模式，所有节点执行。保证所有节点有完整的非CV，stream数据。  
  
![pic](20180314_04_pic_006.jpg)  
  
建议应用程序通过连接池连到HAPROXY或其他负载均衡软件，然后再连接各个pipelinedb node。实现最大吞吐。建议使用HAPROXY或LVS来实现负载均衡，当然现在的jdbc , libpq或其他驱动很多都支持了负载均衡的配置。  
  
使用pgbouncer来实现连接池的功能，pgbouncer纯C开发，是目前PostgreSQL比较高效的一款连接池。  
  
  
## shard, node概念  
node指PP的数据节点。  
  
shard是逻辑概念，在创建cv时，需要指定这个CV需要开启多少个shard。  
  
注意，为了达到最高横向扩展效率，对于同一个CV，每个NODE只负责它一个SHARD。因此一个CV最多可以创建多少SHARD呢？（当然是不能超过NODE个数了）  
  
![pic](20180314_04_pic_005.jpg)  
  
定义一个CV跨多少个SHARD的两种方法:  
  
```  
CREATE CONTINUOUS VIEW v WITH (num_shards=8) AS  
  SELECT x::integer, COUNT(*) FROM stream GROUP BY x;  
  
  
CREATE CONTINUOUS VIEW v WITH (shard_factor=50) AS  
  SELECT x::integer, COUNT(*) FROM stream GROUP BY x;  
```  
  
当使用shard_factor设置时，实际上给出的是一个百分比值，取值1-100，因此对于16个node的集群，50的意思就是8个shard。  
  
在CV创建时，会生成元数据（CV在哪里，分片策略是什么（读优化还是写优化），等），元数据结构如下：  
  
```  
\d pipeline_cluster.shards  
  View "pipeline_cluster.shards"  
     Column      |   Type   | Modifiers  
-----------------+----------+-----------  
 shard           | text     |  
 owner           | text     |  
 continuous_view | text     |  
 shard_id        | smallint |  
 shard_type      | text     |  
 sharding_policy | text     |  
 is_local        | boolean  |  
 balanced        | boolean  |  
 reachable       | boolean  |  
```  
  
## 数据路由策略  
创建CV时，使用sharding_policy指定数据路由策略。  
  
### 读优化  
  
一个CV中，同一个聚合分组的数据会路由到某一个节点，读取时不需要二次合并。但是写入可能会涉及数据重分发（当然这个动作是pipelinedb shard节点透明的完成的，应用不感知，只是写性能肯定有所下降）  
  
注意网络上分发的是聚合后的数据，即一批聚合后的结果（不同的分组，分发到对应的shard）。  
  
What this means is that only the aggregate result of incoming rows actually needs to be routed. This is designed to both minimize network overhead and distribute work.  
  
![pic](20180314_04_pic_002.jpg)  
  
  
```  
INSERT INTO stream (x) VALUES (0), (0), (0), (0), (0);  
```  
  
Assuming that the worker process reads these five rows all at once fast enough, only the aggregate row (0, 5) would be routed to the grouping’s designated node, and subsequently combined with on-disk data as usual.  
  
```  
CREATE CONTINUOUS VIEW v0 WITH (sharding_policy='read_optimized') AS  
  SELECT x::integer, COUNT(*) FROM stream GROUP BY x;  
```  
  
### 写优化  
  
每个shard管自己的CV，因此同一个维度的数据可能出现在多个SHARD中。写性能达到最大吞吐。  
  
仅仅当本地节点不包含目标CV的任何shard时，才需要分发聚合后的部分结果。（关注一下代码：是一批中间结果打散随机分发到所有shard，还是随机选一个shard，将整一批中间结果全部发给这个shard？）  
  
Routing is then only necessary if a node that produces a partial aggregate result for a given group does not have any shards for the group’s continuous view.   
  
![pic](20180314_04_pic_003.jpg)  
  
```  
CREATE CONTINUOUS VIEW v1 WITH (sharding_policy='write_optimized') AS  
  SELECT x::integer, COUNT(*) FROM stream GROUP BY x;  
```  
  
### 读合并  
  
采用写优化模式时，为了保证数据的完整性，读时需要合并。因此PIPELINEDB需要支持所有的CV都具备合并能力，包括count,avg,sum等常见指标，以及CMS-TOP，HLL等概率指标数据类型的合并，还好这些概率类型目前都是支持同类UNION的。  
  
  
![pic](20180314_04_pic_004.jpg)  
  
```  
CREATE CONTINUOUS VIEW v WITH (sharding_policy='write_optimized', shards=16) AS  
  SELECT x::integer, AVG(y::integer) FROM stream GROUP BY x;  
```  
  
Since it uses a write_optimized grouping policy, multiple local copies for each grouped average may exist. At read time, these groups would be combined with no loss of information, producing a finalized result with exactly one row per group.  
  
http://docs.pipelinedb.com/aggregates.html#combine  
  
## HA、分片负载均衡  
http://enterprise.pipelinedb.com/docs/high-availability.html  
  
内部使用PostgreSQL的异步逻辑订阅功能(logical decoding)，实现cv结果的多副本，在创建cv时通过num_replicas指定副本数。  
  
注意这里使用的是异步复制，所以在创建CV时，会指定primary_node，它是主，数据从它复制给其他的shard。  
  
```  
CREATE CONTINUOUS VIEW v WITH (num_shards=3, num_replicas=2) AS  
  SELECT x::integer, COUNT(*) FROM stream GROUP BY x;  
```  
  
当设置了多个副本时，可以实现分片负载均衡：  
  
读负载均衡：设置pipeline_cluster.primary_only=false，当primary shard异常时，会读取它的副本shard。  
  
写负载均衡，读优化模式，将所有副本加入hash ring，在一个ring中有一个shard所在NODE来计算。如果这个SHARD挂了，则调取该shard的下一个副本。  
  
写负载均衡，写优化模式，没什么作用，因为写优化模式本身就是优先落本地的。  
  
## 渊源  
pipelinedb与citus有一定的渊源，cluster版本，应该借鉴了很多citus pg_shard插件的设计理念，甚至可能有大量代码复用。  
  
https://www.citusdata.com/product  
  
https://github.com/citusdata/pg_shard  
  
https://github.com/citusdata/citus  
  
  
## 小结  
pipelinedb cluster定位是一个分布式流式计算引擎。支持了良好的计算扩展性、可靠性、负载均衡能力，同时一个CV支持跨SHARD计算，支持读优化模式，支持写优化模式，同时支持写优化模式的自动合并聚合计算结果，是一个非常棒的分布式流计算引擎。    
  
通过分发中间聚合结果，优化了网络开销。   
  
  
## 参考  
http://enterprise.pipelinedb.com/docs/index.html  
  
http://docs.pipelinedb.com/aggregates.html#combine  
  
https://www.citusdata.com/product  
  
  
<a rel="nofollow" href="http://info.flagcounter.com/h9V1"  ><img src="http://s03.flagcounter.com/count/h9V1/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_12/viewers_0/labels_0/pageviews_0/flags_0/"  alt="Flag Counter"  border="0"  ></a>  
  
